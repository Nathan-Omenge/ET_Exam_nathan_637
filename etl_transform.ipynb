{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f37ee576",
   "metadata": {},
   "source": [
    "# Data Transformation\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "This notebook demonstrates the **Transform (T)** phase of the ETL process.  \n",
    "The goal of this phase is to **clean, standardize, enrich, and prepare** both the *raw* and *incremental* datasets for analytical use in later stages.\n",
    "\n",
    "The transformation steps applied here include:\n",
    "\n",
    "1. Handling missing values  \n",
    "2. Removing duplicates  \n",
    "3. Standardizing date and text formats  \n",
    "4. Enriching the data with derived features  \n",
    "5. Categorizing continuous variables  \n",
    "\n",
    "Each transformation is demonstrated with **before-and-after** outputs and brief discussions to show its effect on the dataset.  \n",
    "The final, cleaned datasets will be saved in the `/transformed/` directory for subsequent use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42456e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataset shape: (14640, 15)\n",
      "Incremental dataset shape: (2000, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52-08:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59-08:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                        0.0  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold      name negativereason_gold  retweet_count  \\\n",
       "0                    NaN   cairdin                 NaN              0   \n",
       "1                    NaN  jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52-08:00            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59-08:00            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570310600460525568</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.6292</td>\n",
       "      <td>Flight Booking Problems</td>\n",
       "      <td>0.3146</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jhazelnut</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@USAirways  is there a better time to call? My...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:53:37-08:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570310144459972608</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GAKotsch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@USAirways and when will one of these agents b...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:51:48-08:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Atlantic Time (Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570310600460525568          negative                        0.6292   \n",
       "1  570310144459972608          negative                        1.0000   \n",
       "\n",
       "            negativereason  negativereason_confidence     airline  \\\n",
       "0  Flight Booking Problems                     0.3146  US Airways   \n",
       "1   Customer Service Issue                     1.0000  US Airways   \n",
       "\n",
       "  airline_sentiment_gold       name negativereason_gold  retweet_count  \\\n",
       "0                    NaN  jhazelnut                 NaN              0   \n",
       "1                    NaN   GAKotsch                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0  @USAirways  is there a better time to call? My...         NaN   \n",
       "1  @USAirways and when will one of these agents b...         NaN   \n",
       "\n",
       "               tweet_created tweet_location           user_timezone  \n",
       "0  2015-02-24 11:53:37-08:00            NaN                     NaN  \n",
       "1  2015-02-24 11:51:48-08:00            NaN  Atlantic Time (Canada)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Library Imports and Data Loading\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define data directory\n",
    "data_dir = \"data\"\n",
    "\n",
    "# Define file paths\n",
    "raw_path = os.path.join(data_dir, \"raw_data.csv\")\n",
    "incremental_path = os.path.join(data_dir, \"incremental_data.csv\")\n",
    "\n",
    "# Load validated datasets\n",
    "df_raw = pd.read_csv(raw_path)\n",
    "df_incremental = pd.read_csv(incremental_path)\n",
    "\n",
    "# Preview datasets for confirmation\n",
    "print(\"Raw dataset shape:\", df_raw.shape)\n",
    "print(\"Incremental dataset shape:\", df_incremental.shape)\n",
    "\n",
    "display(df_raw.head(2))\n",
    "display(df_incremental.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49b9c7d",
   "metadata": {},
   "source": [
    "## 2. Data Transformations\n",
    "\n",
    "In this section, I perform a series of transformations to ensure both datasets are clean, standardized, and ready for analysis.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55501c9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Transformation 1: Handling Missing Values\n",
    "\n",
    "The dataset contains missing entries in several columns — notably `negativereason` and `tweet_location`.  \n",
    "\n",
    "To maintain record completeness while preserving interpretability:\n",
    "\n",
    "- Missing values in `negativereason` are replaced with `\"Unknown\"`,since this field represents the reason behind a negative sentiment and can logically default to “Unknown” when unspecified.\n",
    "- Missing values in `tweet_location` and `user_timezone` are left as-is because imputing them could introduce false geographic information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d368792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before transformation:\n",
      "negativereason                5462\n",
      "negativereason_confidence     4118\n",
      "airline_sentiment_gold       14600\n",
      "negativereason_gold          14608\n",
      "tweet_coord                  13621\n",
      "tweet_location                4733\n",
      "user_timezone                 4820\n",
      "dtype: int64\n",
      "\n",
      "Missing values after transformation:\n",
      "negativereason_confidence     4118\n",
      "airline_sentiment_gold       14600\n",
      "negativereason_gold          14608\n",
      "tweet_coord                  13621\n",
      "tweet_location                4733\n",
      "user_timezone                 4820\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- BEFORE ---\n",
    "print(\"Missing values before transformation:\")\n",
    "print(df_raw.isnull().sum()[df_raw.isnull().sum() > 0])\n",
    "\n",
    "# Apply transformation\n",
    "cols_to_fill = ['negativereason']\n",
    "for col in cols_to_fill:\n",
    "    df_raw[col] = df_raw[col].fillna('Unknown')\n",
    "    df_incremental[col] = df_incremental[col].fillna('Unknown')\n",
    "\n",
    "# --- AFTER ---\n",
    "print(\"\\nMissing values after transformation:\")\n",
    "print(df_raw.isnull().sum()[df_raw.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9adeb75",
   "metadata": {},
   "source": [
    "### **Discussion:**  \n",
    "After the transformation, all missing values in the `negativereason` column were successfully replaced with `\"Unknown\"`.  \n",
    "The other missing fields (`tweet_location`, `user_timezone`) were retained in their original form to prevent misleading imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8550d81",
   "metadata": {},
   "source": [
    "---\n",
    "### Transformation 2: Removing Duplicates\n",
    "\n",
    "Duplicate records can distort summary statistics, bias analyses, and inflate record counts.  \n",
    "Since each tweet in the dataset is uniquely identified by its `tweet_id`, we use this field to detect and remove duplicates.  \n",
    "\n",
    "This ensures that every observation in the dataset represents a distinct tweet event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61a363a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate records before transformation:\n",
      "155 duplicates in raw dataset\n",
      "147 duplicates in incremental dataset\n",
      "\n",
      "Duplicate records after transformation:\n",
      "0 duplicates in raw dataset\n",
      "0 duplicates in incremental dataset\n",
      "\n",
      "Updated dataset shapes:\n",
      "Raw: (14485, 15)\n",
      "Incremental: (1853, 15)\n"
     ]
    }
   ],
   "source": [
    "# --- BEFORE ---\n",
    "print(\"Duplicate records before transformation:\")\n",
    "print(df_raw.duplicated(subset='tweet_id').sum(), \"duplicates in raw dataset\")\n",
    "print(df_incremental.duplicated(subset='tweet_id').sum(), \"duplicates in incremental dataset\")\n",
    "\n",
    "# Apply transformation\n",
    "df_raw = df_raw.drop_duplicates(subset='tweet_id').reset_index(drop=True)\n",
    "df_incremental = df_incremental.drop_duplicates(subset='tweet_id').reset_index(drop=True)\n",
    "\n",
    "# --- AFTER ---\n",
    "print(\"\\nDuplicate records after transformation:\")\n",
    "print(df_raw.duplicated(subset='tweet_id').sum(), \"duplicates in raw dataset\")\n",
    "print(df_incremental.duplicated(subset='tweet_id').sum(), \"duplicates in incremental dataset\")\n",
    "\n",
    "print(\"\\nUpdated dataset shapes:\")\n",
    "print(\"Raw:\", df_raw.shape)\n",
    "print(\"Incremental:\", df_incremental.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e204ff",
   "metadata": {},
   "source": [
    "After removing duplicates based on `tweet_id`, each record now uniquely represents one tweet.  \n",
    "This ensures data integrity and prevents skewed analyses in later stages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc3696",
   "metadata": {},
   "source": [
    "---\n",
    "### Transformation 3: Standardizing Datetime Format\n",
    "\n",
    "The `tweet_created` column is currently stored as a string, which limits its usability for time-based analysis.  \n",
    "To standardize this field:\n",
    "\n",
    "- The column is converted to `datetime` format using `pd.to_datetime()`.  \n",
    "- A new column, `tweet_date`, is extracted to represent only the date portion of each record.  \n",
    "\n",
    "This transformation enables accurate temporal grouping, filtering, and trend analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaf6f0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before transformation:\n",
      "0    2015-02-24 11:35:52-08:00\n",
      "1    2015-02-24 11:15:59-08:00\n",
      "2    2015-02-24 11:15:48-08:00\n",
      "Name: tweet_created, dtype: object\n",
      "\n",
      "Column data type: object\n",
      "\n",
      "After transformation:\n",
      "              tweet_created  tweet_date\n",
      "0 2015-02-24 11:35:52-08:00  2015-02-24\n",
      "1 2015-02-24 11:15:59-08:00  2015-02-24\n",
      "2 2015-02-24 11:15:48-08:00  2015-02-24\n",
      "\n",
      "Column data type after: datetime64[ns, UTC-08:00]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- BEFORE ---\n",
    "print(\"Before transformation:\")\n",
    "print(df_raw['tweet_created'].head(3))\n",
    "print(\"\\nColumn data type:\", df_raw['tweet_created'].dtype)\n",
    "\n",
    "# Apply transformation\n",
    "df_raw['tweet_created'] = pd.to_datetime(df_raw['tweet_created'], errors='coerce')\n",
    "df_incremental['tweet_created'] = pd.to_datetime(df_incremental['tweet_created'], errors='coerce')\n",
    "\n",
    "# Extract date component\n",
    "df_raw['tweet_date'] = df_raw['tweet_created'].dt.date\n",
    "df_incremental['tweet_date'] = df_incremental['tweet_created'].dt.date\n",
    "\n",
    "# --- AFTER ---\n",
    "print(\"\\nAfter transformation:\")\n",
    "print(df_raw[['tweet_created', 'tweet_date']].head(3))\n",
    "print(\"\\nColumn data type after:\", df_raw['tweet_created'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011a1718",
   "metadata": {},
   "source": [
    "**Discussion:**  \n",
    "The `tweet_created` column has been successfully converted from string format to a proper `datetime` object, and a new `tweet_date` field was created to represent the calendar date of each tweet.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe7ce42",
   "metadata": {},
   "source": [
    "---\n",
    "### Transformation 4: Enriching the Dataset with Tweet Length\n",
    "\n",
    "To enrich the dataset with an additional analytical feature, I created a new column called `tweet_length`.  \n",
    "This column captures the number of characters in each tweet (based on the `text` field).  \n",
    "\n",
    "The metric provides insight into how expressive or concise user feedback tends to be,  \n",
    "and it can later be used in exploratory visualizations or feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b78ea193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before transformation:\n",
      "                                                text\n",
      "0                @VirginAmerica What @dhepburn said.\n",
      "1  @VirginAmerica plus you've added commercials t...\n",
      "2  @VirginAmerica I didn't today... Must mean I n...\n",
      "\n",
      "After transformation:\n",
      "                                                text  tweet_length\n",
      "0                @VirginAmerica What @dhepburn said.            35\n",
      "1  @VirginAmerica plus you've added commercials t...            72\n",
      "2  @VirginAmerica I didn't today... Must mean I n...            71\n",
      "\n",
      "Incremental dataset sample:\n",
      "                                                text  tweet_length\n",
      "0  @USAirways  is there a better time to call? My...           128\n",
      "1  @USAirways and when will one of these agents b...            67\n",
      "2  @JetBlue Yesterday on my way from EWR to FLL j...           115\n"
     ]
    }
   ],
   "source": [
    "# Transformation 4: Enriching the Dataset with Tweet Length\n",
    "\n",
    "# --- BEFORE ---\n",
    "print(\"Before transformation:\")\n",
    "print(df_raw[['text']].head(3))\n",
    "\n",
    "# Apply transformation\n",
    "df_raw['tweet_length'] = df_raw['text'].astype(str).apply(len)\n",
    "df_incremental['tweet_length'] = df_incremental['text'].astype(str).apply(len)\n",
    "\n",
    "# --- AFTER ---\n",
    "print(\"\\nAfter transformation:\")\n",
    "print(df_raw[['text', 'tweet_length']].head(3))\n",
    "\n",
    "print(\"\\nIncremental dataset sample:\")\n",
    "print(df_incremental[['text', 'tweet_length']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3b2a0c",
   "metadata": {},
   "source": [
    "**Discussion:**  \n",
    "Both datasets have been enriched with a new variable, `tweet_length`, representing the total number of characters per tweet.  \n",
    "This transformation enables future correlation analysis between tweet sentiment and number of words used in the tweet for example.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e708e36",
   "metadata": {},
   "source": [
    "---\n",
    "### Transformation 5: Deriving Sentiment Category Flags\n",
    "\n",
    "To allow for future numerical analysis or potential model training,  \n",
    "I created three binary indicator columns representing the sentiment categories:\n",
    "\n",
    "- `sentiment_positive` → 1 if sentiment is *positive*, else 0  \n",
    "- `sentiment_neutral` → 1 if sentiment is *neutral*, else 0  \n",
    "- `sentiment_negative` → 1 if sentiment is *negative*, else 0  \n",
    "\n",
    "This transformation converts the text into a structured numeric format for easier aggregation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9846132e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before transformation:\n",
      "  airline_sentiment\n",
      "0           neutral\n",
      "1          positive\n",
      "2           neutral\n",
      "\n",
      "After transformation:\n",
      "  airline_sentiment  sentiment_positive  sentiment_neutral  sentiment_negative\n",
      "0           neutral                   0                  1                   0\n",
      "1          positive                   1                  0                   0\n",
      "2           neutral                   0                  1                   0\n"
     ]
    }
   ],
   "source": [
    "# --- BEFORE ---\n",
    "print(\"Before transformation:\")\n",
    "print(df_raw[['airline_sentiment']].head(3))\n",
    "\n",
    "# Apply transformation\n",
    "for df in [df_raw, df_incremental]:\n",
    "    df['sentiment_positive'] = (df['airline_sentiment'].str.lower() == 'positive').astype(int)\n",
    "    df['sentiment_neutral'] = (df['airline_sentiment'].str.lower() == 'neutral').astype(int)\n",
    "    df['sentiment_negative'] = (df['airline_sentiment'].str.lower() == 'negative').astype(int)\n",
    "\n",
    "# --- AFTER ---\n",
    "print(\"\\nAfter transformation:\")\n",
    "print(df_raw[['airline_sentiment', 'sentiment_positive', 'sentiment_neutral', 'sentiment_negative']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f7c1f4",
   "metadata": {},
   "source": [
    "**Discussion:**  \n",
    "The categorical variable `airline_sentiment` has been converted into three binary indicator columns.  \n",
    "Each column now explicitly identifies whether a record represents a positive, neutral, or negative tweet.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a17856",
   "metadata": {},
   "source": [
    "---\n",
    "### Transformation 6: Standardizing Airline Names\n",
    "\n",
    "During extraction, the `airline` field contained valid names such as *Virgin America*, *United*, and *American*.  \n",
    "However, inconsistencies in capitalization or spacing can cause grouping and aggregation errors.\n",
    "\n",
    "To standardize this column, all airline names were converted to title case using the `str.title()` method.  \n",
    "This ensures consistent formatting across both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba323f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before transformation:\n",
      "['Virgin America' 'United' 'Southwest' 'Delta' 'US Airways' 'American']\n",
      "\n",
      "After transformation:\n",
      "['Virgin America' 'United' 'Southwest' 'Delta' 'Us Airways' 'American']\n"
     ]
    }
   ],
   "source": [
    "# --- BEFORE ---\n",
    "print(\"Before transformation:\")\n",
    "print(df_raw['airline'].unique())\n",
    "\n",
    "# Apply transformation\n",
    "for df in [df_raw, df_incremental]:\n",
    "    df['airline'] = df['airline'].str.strip().str.title()\n",
    "\n",
    "# --- AFTER ---\n",
    "print(\"\\nAfter transformation:\")\n",
    "print(df_raw['airline'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de307232",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Saving Transformed Data\n",
    "\n",
    "After applying all transformations, the final step involves saving the processed versions of both datasets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a76da3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed datasets saved to 'transformed/' directory.\n",
      "Files:\n",
      "- transformed/transformed_raw.csv\n",
      "- transformed/transformed_incremental.csv\n"
     ]
    }
   ],
   "source": [
    "# 3. Saving Transformed Data\n",
    "\n",
    "# Ensure transformed directory exists\n",
    "os.makedirs(\"transformed\", exist_ok=True)\n",
    "\n",
    "# Define output paths\n",
    "raw_transformed_path = os.path.join(\"transformed\", \"transformed_raw.csv\")\n",
    "incremental_transformed_path = os.path.join(\"transformed\", \"transformed_incremental.csv\")\n",
    "\n",
    "# Save transformed datasets\n",
    "df_raw.to_csv(raw_transformed_path, index=False)\n",
    "df_incremental.to_csv(incremental_transformed_path, index=False)\n",
    "\n",
    "# Verify save\n",
    "print(\"Transformed datasets saved to 'transformed/' directory.\")\n",
    "print(\"Files:\")\n",
    "print(f\"- {raw_transformed_path}\")\n",
    "print(f\"- {incremental_transformed_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "etl_exam_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
